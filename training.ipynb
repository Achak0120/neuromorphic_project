{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27708ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 100000 rows to firetest_data.csv\n",
      "Fire rows: 50193\n",
      "No-fire rows: 49807\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import spikegen\n",
    "from snntorch import surrogate\n",
    "from snntorch import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import tensorflow as tf\n",
    "from forest_fire_snn import SNN\n",
    "import pandas as pd\n",
    "# Create an instance of SNN Object\n",
    "net = SNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025362e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Loss=0.2297 | Acc=0.906\n",
      "Epoch 2/25 | Loss=0.2108 | Acc=0.915\n",
      "Epoch 3/25 | Loss=0.2012 | Acc=0.918\n",
      "Epoch 4/25 | Loss=0.1996 | Acc=0.918\n",
      "Epoch 5/25 | Loss=0.1940 | Acc=0.920\n",
      "Epoch 6/25 | Loss=0.1925 | Acc=0.920\n",
      "Epoch 7/25 | Loss=0.1901 | Acc=0.921\n",
      "Epoch 8/25 | Loss=0.1907 | Acc=0.920\n",
      "Epoch 9/25 | Loss=0.1887 | Acc=0.921\n",
      "Epoch 10/25 | Loss=0.1862 | Acc=0.922\n",
      "Epoch 11/25 | Loss=0.1860 | Acc=0.922\n",
      "Epoch 12/25 | Loss=0.1882 | Acc=0.921\n",
      "Epoch 13/25 | Loss=0.1880 | Acc=0.922\n",
      "Epoch 14/25 | Loss=0.1880 | Acc=0.922\n",
      "Epoch 15/25 | Loss=0.1886 | Acc=0.921\n",
      "Epoch 16/25 | Loss=0.1881 | Acc=0.921\n",
      "Epoch 17/25 | Loss=0.1872 | Acc=0.921\n",
      "Epoch 18/25 | Loss=0.1857 | Acc=0.922\n",
      "Epoch 19/25 | Loss=0.1871 | Acc=0.921\n",
      "Epoch 20/25 | Loss=0.1893 | Acc=0.921\n",
      "Epoch 21/25 | Loss=0.1882 | Acc=0.921\n",
      "Epoch 22/25 | Loss=0.1871 | Acc=0.921\n",
      "Epoch 23/25 | Loss=0.1871 | Acc=0.922\n",
      "Epoch 24/25 | Loss=0.1865 | Acc=0.921\n",
      "Epoch 25/25 | Loss=0.1868 | Acc=0.922\n"
     ]
    }
   ],
   "source": [
    "# Prepare CSV data\n",
    "df = pd.read_csv('firetest_data.csv')\n",
    "features = df[['Temp', 'Audio', 'Humidity', 'CO2']]\n",
    "labels = df['Fire']\n",
    "\n",
    "# Normalize features\n",
    "df['Temp'] = df['Temp'] / 100\n",
    "df['Humidity'] = df['Humidity'] / 100\n",
    "df['CO2'] = df['CO2'] / 5000\n",
    "\n",
    "X = df[['Temp', 'Audio', 'Humidity', 'CO2']]\n",
    "Y = df['Fire'].to_numpy()\n",
    "\n",
    "X_np = X.to_numpy()\n",
    "Y_np = Y\n",
    "X_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "X_tensor = X_tensor.clamp(0,1)\n",
    "Y_tensor = torch.tensor(Y_np, dtype=torch.long)\n",
    "\n",
    "# Convert features to spike trains\n",
    "num_steps = 25\n",
    "def to_spike_trains(X_tensor, num_steps=25):\n",
    "    num_samples, num_features = X_tensor.shape\n",
    "    spike_data = torch.zeros((num_samples, num_steps, num_features))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        for t in range(num_steps):\n",
    "            spike_data[i, t] = torch.bernoulli(X_tensor[i])\n",
    "    return spike_data\n",
    "\n",
    "fire_train = to_spike_trains(X_tensor, num_steps=num_steps)\n",
    "\n",
    "class FireDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, spike_data, labels):\n",
    "        self.spike_data = spike_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.spike_data[idx], self.labels[idx]\n",
    "    \n",
    "dataset = FireDataset(fire_train, Y_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
    "    \n",
    "    for data, targets in train_loader:\n",
    "        data = data.permute(1, 0, 2)\n",
    "        utils.reset(net)\n",
    "        spk_rec, mem_rec = net(data)\n",
    "        logits = spk_rec.sum(dim=0)\n",
    "        \n",
    "        loss = loss_fn(logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        bs = targets.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_correct += (logits.argmax(dim=1) == targets).sum().item()\n",
    "        total_n += bs\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss={total_loss/total_n:.4f} | Acc={total_correct/total_n:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef37e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.1812, acc=0.925 | Val: loss=0.1867, acc=0.923\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "from snntorch import utils\n",
    "\n",
    "def make_train_val_loaders(dataset, batch_size=64, val_frac=0.2, seed=42):\n",
    "    n = len(dataset)\n",
    "    val_n = int(val_frac * n)\n",
    "    train_n = n - val_n\n",
    "\n",
    "    train_ds, val_ds = random_split(\n",
    "        dataset,\n",
    "        [train_n, val_n],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def eval_acc_loss(net, loader, loss_fn, device=None):\n",
    "    \"\"\"\n",
    "    net: your SNN model\n",
    "    loader: train_loader or val_loader\n",
    "    loss_fn: e.g. nn.CrossEntropyLoss()\n",
    "    device: optional torch.device; if provided, moves tensors to device\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            # data shape from dataset: (batch, time, features)\n",
    "            #(time, batch, features)\n",
    "            data = data.permute(1, 0, 2)\n",
    "\n",
    "            if device is not None:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "            utils.reset(net)\n",
    "            spk_rec, mem_rec = net(data)   # spk_rec: (time, batch, num_outputs)\n",
    "\n",
    "            logits = spk_rec.sum(dim=0)    # (batch, num_outputs)\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            bs = targets.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_correct += (logits.argmax(dim=1) == targets).sum().item()\n",
    "            total_n += bs\n",
    "\n",
    "    return total_loss / total_n, total_correct / total_n\n",
    "\n",
    "\n",
    "train_loader, val_loader = make_train_val_loaders(dataset, batch_size=64, val_frac=0.2, seed=42)\n",
    "train_loss, train_acc = eval_acc_loss(net, train_loader, loss_fn)\n",
    "val_loss, val_acc     = eval_acc_loss(net, val_loader, loss_fn)\n",
    "print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.3f} | Val: loss={val_loss:.4f}, acc={val_acc:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61579d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model to file path\n",
    "import torch\n",
    "\n",
    "PATH = 'trained_snn_model.pth'\n",
    "\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8262c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
