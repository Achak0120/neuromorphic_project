{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27708ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import spikegen\n",
    "from snntorch import surrogate\n",
    "from snntorch import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import csv\n",
    "from forest_fire_snn import SNN\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025362e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m             spike_data[i, t] = torch.bernoulli(X_tensor[i])\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m spike_data\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m fire_train = \u001b[43mto_spike_trains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFireDataset\u001b[39;00m(torch.utils.data.Dataset):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike_data, labels):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mto_spike_trains\u001b[39m\u001b[34m(X_tensor, num_steps)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         spike_data[i, t] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m spike_data\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create an instance of SNN Object\n",
    "net = SNN()\n",
    "\n",
    "# Prepare CSV data\n",
    "df = pd.read_csv('firetest_data.csv')\n",
    "features = df[['Temp', 'Audio', 'Humidity', 'CO2', 'LATITUDE', 'LONGITUDE']]\n",
    "labels = df['Fire']\n",
    "\n",
    "# Normalize features\n",
    "df['Temp'] = df['Temp'] / 100\n",
    "df['Humidity'] = df['Humidity'] / 100\n",
    "df['CO2'] = df['CO2'] / 5000\n",
    "df['LATITUDE'] = (df['LATITUDE'] - 25) / 45\n",
    "df['LONGITUDE'] = (df['LONGITUDE'] + 125) / 65\n",
    "\n",
    "X = df[['Temp', 'Audio', 'Humidity', 'CO2', 'LATITUDE', 'LONGITUDE']]\n",
    "Y = df['Fire'].to_numpy()\n",
    "\n",
    "X_np = X.to_numpy()\n",
    "Y_np = Y\n",
    "X_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_np, dtype=torch.long)\n",
    "\n",
    "# Convert features to spike trains\n",
    "num_steps = 25\n",
    "def to_spike_trains(X_tensor, num_steps=25):\n",
    "    num_samples, num_features = X_tensor.shape\n",
    "    spike_data = torch.zeros((num_samples, num_steps, num_features))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        for t in range(num_steps):\n",
    "            spike_data[i, t] = torch.bernoulli(X_tensor[i])\n",
    "    return spike_data\n",
    "\n",
    "fire_train = to_spike_trains(X_tensor, num_steps=num_steps)\n",
    "\n",
    "class FireDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, spike_data, labels):\n",
    "        self.spike_data = spike_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.spike_data[idx], self.labels[idx]\n",
    "    \n",
    "dataset = FireDataset(fire_train, Y_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, targets in train_loader:\n",
    "        data = data.permute(1,0,2)\n",
    "        \n",
    "        spk_rec, mem_rec = net(data)\n",
    "        \n",
    "        logits = spk_rec.sum(dim=0)\n",
    "        \n",
    "        loss = loss_fn(logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0eac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
